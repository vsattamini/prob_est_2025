# Guia Completo do Projeto - Parte 1: VisÃ£o Geral

**PÃºblico-alvo:** Mestrandos em CiÃªncia da ComputaÃ§Ã£o com conhecimento bÃ¡sico de estatÃ­stica e lÃ³gica fuzzy

**Objetivo:** Explicar em detalhes minuciosos cada processo, indicador e mÃ©todo utilizado no projeto de detecÃ§Ã£o de textos gerados por LLMs em portuguÃªs.

---

## 1. IntroduÃ§Ã£o ao Problema

### 1.1 O Que Estamos Tentando Resolver?

**Problema Central:** Dado um texto em portuguÃªs brasileiro, queremos determinar se ele foi escrito por um humano ou gerado por um modelo de linguagem (LLM) como ChatGPT, GPT-4, etc.

**Por que isso importa?**
- **EducaÃ§Ã£o:** Detectar trabalhos acadÃªmicos gerados por IA
- **Integridade cientÃ­fica:** Identificar artigos ou seÃ§Ãµes escritas por LLMs
- **ModeraÃ§Ã£o de conteÃºdo:** Detectar spam ou desinformaÃ§Ã£o gerada em massa
- **Forense digital:** AtribuiÃ§Ã£o de autoria em investigaÃ§Ãµes

### 1.2 Nossa Abordagem em Duas Frentes

Este projeto usa **duas metodologias complementares** aplicadas aos **mesmos dados**:

#### **Abordagem 1: AnÃ¡lise EstatÃ­stica** (paper_stat)
- Usa testes estatÃ­sticos clÃ¡ssicos (Mann-Whitney U, Cliff's delta)
- Aplica modelos de classificaÃ§Ã£o tradicionais (LDA, RegressÃ£o LogÃ­stica)
- **Vantagens:** Alta precisÃ£o (97% AUC), rigor matemÃ¡tico estabelecido
- **Desvantagens:** Menos interpretÃ¡vel ("caixa preta")

#### **Abordagem 2: LÃ³gica Fuzzy** (paper_fuzzy)
- Usa conjuntos fuzzy e funÃ§Ãµes de pertinÃªncia triangulares
- Cria regras interpretÃ¡veis baseadas em caracterÃ­sticas estilomÃ©tricas
- **Vantagens:** Totalmente interpretÃ¡vel, robustez excepcional (variÃ¢ncia 3-4Ã— menor)
- **Desvantagens:** Menor precisÃ£o (89% AUC) - "custo da interpretabilidade"

**Importante:** Ambas as abordagens usam **exatamente as mesmas 10 caracterÃ­sticas estilomÃ©tricas** extraÃ­das do texto. A diferenÃ§a estÃ¡ em **como** essas caracterÃ­sticas sÃ£o usadas para classificaÃ§Ã£o.

---

## 2. Pipeline Geral do Projeto

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETAPA 1: COLETA DE DADOS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Corpus original: 1.295.958 textos (98% humano, 2% LLM)      â”‚
â”‚ â€¢ Fontes: BrWaC, BoolQ, ShareGPT-PT, Canarim, IMDB traduÃ§Ãµes   â”‚
â”‚ â€¢ Resultado: arquivo balanced.csv (3.2 GB)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                ETAPA 2: AMOSTRAGEM E BALANCEAMENTO              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ EstratificaÃ§Ã£o: 50.000 humano + 50.000 LLM = 100.000 total  â”‚
â”‚ â€¢ MÃ©todo: downsampling (maioria) + upsampling (minoria)       â”‚
â”‚ â€¢ Seed: 42 (para reprodutibilidade)                           â”‚
â”‚ â€¢ Resultado: balanced_sample_100k.csv (257 MB)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ETAPA 3: EXTRAÃ‡ÃƒO DE CARACTERÃSTICAS               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ CÃ³digo: src/features.py                                      â”‚
â”‚ â€¢ Entrada: texto bruto em portuguÃªs                           â”‚
â”‚ â€¢ SaÃ­da: 10 mÃ©tricas numÃ©ricas por texto                      â”‚
â”‚ â€¢ Resultado: features_100k.csv (17 MB)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ETAPA 4A: ANÃLISE ESTATÃSTICA (paper_stat)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Testes de hipÃ³tese: Mann-Whitney U                          â”‚
â”‚ â€¢ Tamanho de efeito: Cliff's delta                            â”‚
â”‚ â€¢ CorreÃ§Ã£o para mÃºltiplos testes: Benjamini-Hochberg (FDR)    â”‚
â”‚ â€¢ AnÃ¡lise multivariada: PCA                                   â”‚
â”‚ â€¢ ClassificaÃ§Ã£o: LDA, RegressÃ£o LogÃ­stica                     â”‚
â”‚ â€¢ ValidaÃ§Ã£o: 5-fold stratified cross-validation               â”‚
â”‚ â€¢ Resultado: 97.03% AUC                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             ETAPA 4B: CLASSIFICAÃ‡ÃƒO FUZZY (paper_fuzzy)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ FunÃ§Ãµes de pertinÃªncia: triangulares baseadas em quantis    â”‚
â”‚ â€¢ Conjuntos fuzzy: "baixo", "mÃ©dio", "alto" para cada feature â”‚
â”‚ â€¢ Sistema de inferÃªncia: Takagi-Sugeno ordem zero             â”‚
â”‚ â€¢ AgregaÃ§Ã£o: mÃ©dia aritmÃ©tica simples                         â”‚
â”‚ â€¢ ValidaÃ§Ã£o: 5-fold stratified cross-validation               â”‚
â”‚ â€¢ Resultado: 89.34% AUC (Â±0.04% std)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ETAPA 5: VISUALIZAÃ‡ÃƒO                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Boxplots das 10 caracterÃ­sticas (humano vs LLM)             â”‚
â”‚ â€¢ Scatter plot PCA (PC1 vs PC2)                               â”‚
â”‚ â€¢ Matriz de correlaÃ§Ã£o (heatmap)                              â”‚
â”‚ â€¢ Curvas ROC (comparando 3 classificadores)                   â”‚
â”‚ â€¢ Curvas Precision-Recall                                     â”‚
â”‚ â€¢ FunÃ§Ãµes de pertinÃªncia fuzzy (10 caracterÃ­sticas Ã— 3 sets)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. As 10 CaracterÃ­sticas EstilomÃ©tricas

**Estilometria** = anÃ¡lise quantitativa do estilo de escrita. Cada texto Ã© transformado em 10 nÃºmeros que capturam diferentes aspectos do estilo.

### 3.1 Grupo 1: EstatÃ­sticas de Frase (3 features)

#### **sent_mean** - Comprimento mÃ©dio de frase
- **O que Ã©:** NÃºmero mÃ©dio de palavras por frase
- **Como calcular:** (total de palavras) Ã· (nÃºmero de frases)
- **Exemplo:** "Eu gosto de cafÃ©. Meu irmÃ£o prefere chÃ¡." â†’ 2 frases, 9 palavras â†’ 4.5 palavras/frase
- **PadrÃ£o observado:** LLMs tendem a ter frases ligeiramente mais longas e uniformes

#### **sent_std** - Desvio padrÃ£o do comprimento de frase
- **O que Ã©:** Medida de variabilidade no tamanho das frases
- **Como calcular:** Desvio padrÃ£o dos comprimentos de todas as frases
- **InterpretaÃ§Ã£o:**
  - `sent_std` alto = frases de tamanhos muito variados (textos humanos)
  - `sent_std` baixo = frases de tamanho uniforme (textos de LLM)
- **PadrÃ£o observado:** Humanos variam mais (Î´ = -0.586, efeito grande)

#### **sent_burst** - Burstiness (explosividade)
- **O que Ã©:** RazÃ£o entre desvio padrÃ£o e mÃ©dia: `sent_burst = sent_std / sent_mean`
- **InterpretaÃ§Ã£o:**
  - `sent_burst` alto = muita variaÃ§Ã£o relativa (ex: frases de 3 e 30 palavras)
  - `sent_burst` baixo = pouca variaÃ§Ã£o relativa (ex: frases de 8 a 12 palavras)
- **Por que importa:** Captura irregularidade estrutural independente da escala
- **PadrÃ£o observado:** Humanos mais "explosivos" (Î´ = -0.599, efeito grande)
- **Leitura sugerida:** Madsen et al. (2005) - "Modeling word burstiness using the Dirichlet distribution"

---

### 3.2 Grupo 2: Diversidade Lexical (3 features)

#### **ttr** - Type-Token Ratio (RelaÃ§Ã£o Tipo-Token)
- **O que Ã©:** RazÃ£o entre palavras Ãºnicas e total de palavras
- **Como calcular:** `TTR = (nÃºmero de palavras Ãºnicas) Ã· (total de palavras)`
- **Exemplo:** "O gato viu o rato" â†’ 5 palavras, 4 Ãºnicas (gato, viu, rato, o) â†’ TTR = 4/5 = 0.8
- **InterpretaÃ§Ã£o:**
  - TTR alto = vocabulÃ¡rio diverso, pouca repetiÃ§Ã£o
  - TTR baixo = vocabulÃ¡rio limitado, muita repetiÃ§Ã£o
- **âš ï¸ LIMITAÃ‡ÃƒO CONHECIDA:** TTR depende do comprimento do texto! Textos mais longos naturalmente tÃªm TTR menor.
- **PadrÃ£o observado:** LLMs tÃªm TTR mais alto (Î´ = +0.636, efeito grande)
- **Leitura sugerida:**
  - Richards (1987) - "Type/Token Ratios: what do they really tell us?" - CrÃ­tica ao TTR
  - McCarthy & Jarvis (2010) - "MTLD, vocd-D, and HD-D" - Alternativas melhores

#### **herdan_c** - C de Herdan
- **O que Ã©:** VersÃ£o normalizada do TTR que reduz dependÃªncia do comprimento
- **FÃ³rmula:** `C = log(V) / log(N)` onde V = vocabulÃ¡rio, N = tokens
- **Como calcular:**
  ```python
  import math
  V = len(set(words))  # palavras Ãºnicas
  N = len(words)       # total de palavras
  herdan_c = math.log(V) / math.log(N)
  ```
- **InterpretaÃ§Ã£o:** Similar ao TTR, mas mais robusto para textos de tamanhos diferentes
- **PadrÃ£o observado:** LLMs tÃªm C maior (Î´ = +0.587)
- **Leitura sugerida:** Herdan (1960) - "Type-token Mathematics"

#### **hapax_prop** - ProporÃ§Ã£o de Hapax Legomena
- **O que sÃ£o hapax legomena:** Palavras que aparecem exatamente uma vez no texto
- **Como calcular:** `hapax_prop = (palavras com frequÃªncia = 1) Ã· (total de palavras)`
- **Exemplo:** "O gato viu o rato preto" â†’ 6 palavras
  - FrequÃªncias: o=2, gato=1, viu=1, rato=1, preto=1
  - Hapax: gato, viu, rato, preto (4 palavras)
  - hapax_prop = 4/6 = 0.667
- **InterpretaÃ§Ã£o:** Mede originalidade e diversidade vocabular
- **PadrÃ£o observado:** LLMs tÃªm mais hapax (Î´ = +0.613) - vocabulÃ¡rio mais diverso
- **CorrelaÃ§Ã£o:** Hapax e TTR sÃ£o fortemente correlacionados (r = 0.87)

---

### 3.3 Grupo 3: Entropia e Estrutura (2 features)

#### **char_entropy** - Entropia de Shannon em nÃ­vel de caractere
- **O que Ã©:** Medida de "surpresa" ou imprevisibilidade na distribuiÃ§Ã£o de caracteres
- **FÃ³rmula:** `H = -Î£ p(c) Ã— logâ‚‚(p(c))` para cada caractere c
- **Como calcular:**
  ```python
  from collections import Counter
  import math

  text = "exemplo de texto"
  freq = Counter(text)
  total = len(text)

  entropy = 0
  for char, count in freq.items():
      p = count / total
      entropy -= p * math.log2(p)
  ```
- **InterpretaÃ§Ã£o:**
  - Entropia alta = distribuiÃ§Ã£o uniforme, imprevisÃ­vel (ex: "abcdefgh")
  - Entropia baixa = distribuiÃ§Ã£o concentrada, previsÃ­vel (ex: "aaaabbbb")
- **Exemplo prÃ¡tico:**
  - "aaaa" â†’ entropia â‰ˆ 0 (totalmente previsÃ­vel)
  - "abcd" â†’ entropia = 2.0 (mÃ¡xima para 4 sÃ­mbolos)
  - Texto real portuguÃªs â†’ entropia â‰ˆ 4.2-4.6 bits
- **PadrÃ£o observado:** Humanos tÃªm entropia MAIOR (Î´ = -0.881, **EFEITO MAIOR DE TODOS**)
  - **Por quÃª?** Humanos cometem erros de digitaÃ§Ã£o, usam pontuaÃ§Ã£o variada, incluem emojis, acentos irregulares
  - LLMs geram texto "limpo" e previsÃ­vel
- **Leitura sugerida:** Shannon (1948) - "A Mathematical Theory of Communication" (paper fundacional da teoria da informaÃ§Ã£o)

#### **func_word_ratio** - ProporÃ§Ã£o de palavras funcionais
- **O que sÃ£o palavras funcionais:** Palavras sem conteÃºdo semÃ¢ntico prÃ³prio (artigos, preposiÃ§Ãµes, pronomes, conjunÃ§Ãµes)
- **Exemplos em portuguÃªs:** "o", "a", "de", "em", "que", "para", "com", "por", "se", "nÃ£o", "mas", "como"
- **Como calcular:** `func_word_ratio = (palavras funcionais) Ã· (total de palavras)`
- **Lista usada no projeto:** ~200 palavras funcionais mais comuns em portuguÃªs
  ```python
  func_words = {"o", "a", "os", "as", "um", "uma", "de", "em", "para",
                "com", "por", "que", "se", "nÃ£o", "mais", ...}
  ```
- **InterpretaÃ§Ã£o:** Mede densidade de estrutura gramatical vs conteÃºdo
- **PadrÃ£o observado:** LLMs usam mais palavras funcionais (Î´ = +0.361, efeito mÃ©dio)
  - **HipÃ³tese:** LLMs treinados em textos formais tendem a estrutura gramatical explÃ­cita
- **Leitura sugerida:**
  - Mosteller & Wallace (1964) - "Inference and Disputed Authorship: The Federalist" - Uso pioneiro de palavras funcionais
  - Eder (2015) - "Does size matter? Authorship attribution, small samples, big problem"

---

### 3.4 Grupo 4: CaracterÃ­sticas de Autoria (2 features)

#### **first_person_ratio** - ProporÃ§Ã£o de pronomes de primeira pessoa
- **O que Ã©:** FraÃ§Ã£o de palavras que sÃ£o pronomes de 1Âª pessoa
- **Pronomes incluÃ­dos:** "eu", "me", "mim", "comigo", "nÃ³s", "nos", "conosco", "meu", "minha", "meus", "minhas", "nosso", "nossa", etc.
- **Como calcular:**
  ```python
  first_person = {"eu", "me", "mim", "comigo", "nÃ³s", "nos", "conosco",
                  "meu", "minha", "meus", "minhas", "nosso", "nossa", ...}
  first_person_ratio = sum(1 for word in tokens if word in first_person) / len(tokens)
  ```
- **InterpretaÃ§Ã£o:** Textos mais pessoais/narrativos vs objetivos/descritivos
- **PadrÃ£o observado:** Efeito **negligenciÃ¡vel** (Î´ = -0.049, p < 0.001 mas irrelevante na prÃ¡tica)
  - **Por quÃª?** Ambos corpus contÃªm mix de textos pessoais e objetivos
  - SignificÃ¢ncia estatÃ­stica (p-valor) â‰  significÃ¢ncia prÃ¡tica (tamanho de efeito)

#### **bigram_repeat_ratio** - ProporÃ§Ã£o de bigramas repetidos
- **O que sÃ£o bigramas:** Pares consecutivos de palavras
- **Exemplo:** "O gato viu o rato" â†’ bigramas: ["O gato", "gato viu", "viu o", "o rato"]
- **Como calcular:**
  ```python
  bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]
  unique_bigrams = len(set(bigrams))
  total_bigrams = len(bigrams)
  bigram_repeat_ratio = 1 - (unique_bigrams / total_bigrams)
  ```
- **InterpretaÃ§Ã£o:**
  - Ratio alto = muitos bigramas repetidos (textos repetitivos)
  - Ratio baixo = poucos bigramas repetidos (textos diversos)
- **PadrÃ£o observado:** Humanos repetem menos bigramas (Î´ = -0.231, efeito pequeno)

---

## 4. Tamanhos de Efeito: InterpretaÃ§Ã£o

Nos testes estatÃ­sticos, usamos **Cliff's delta (Î´)** como medida de tamanho de efeito.

### 4.1 O Que Ã‰ Cliff's Delta?

**DefiniÃ§Ã£o intuitiva:** Se escolhermos aleatoriamente um texto humano e um texto de LLM, qual a probabilidade de que uma caracterÃ­stica seja maior no humano menos a probabilidade de ser maior no LLM?

**FÃ³rmula conceitual:**
```
Î´ = P(X_humano > X_llm) - P(X_humano < X_llm)
```

**Escala de interpretaÃ§Ã£o** (Romano et al. 2006):
- `|Î´| < 0.147` â†’ Efeito **negligenciÃ¡vel** (diferenÃ§a irrelevante)
- `0.147 â‰¤ |Î´| < 0.330` â†’ Efeito **pequeno** (diferenÃ§a detectÃ¡vel mas sutil)
- `0.330 â‰¤ |Î´| < 0.474` â†’ Efeito **mÃ©dio** (diferenÃ§a clara)
- `|Î´| â‰¥ 0.474` â†’ Efeito **grande** (diferenÃ§a substancial)

**Sinal:**
- Î´ **negativo** â†’ LLMs tendem a ter valores **maiores**
- Î´ **positivo** â†’ Humanos tendem a ter valores **maiores**

### 4.2 Resultados do Nosso Projeto

| CaracterÃ­stica | Delta (Î´) | Tamanho de Efeito | InterpretaÃ§Ã£o |
|----------------|-----------|-------------------|---------------|
| char_entropy | **-0.881** | GRANDE | LLMs tÃªm entropia muito maior |
| sent_burst | **-0.599** | GRANDE | Humanos mais "explosivos" |
| ttr | **+0.636** | GRANDE | LLMs mais diversos lexicalmente |
| sent_std | **-0.586** | GRANDE | Humanos variam mais |
| herdan_c | **+0.587** | GRANDE | LLMs vocabulÃ¡rio maior |
| hapax_prop | **+0.613** | GRANDE | LLMs mais palavras Ãºnicas |
| func_word_ratio | **+0.361** | MÃ‰DIO | LLMs usam mais palavras funcionais |
| bigram_repeat_ratio | **-0.231** | PEQUENO | Humanos repetem menos |
| sent_mean | **+0.126** | NEGLIGENCIÃVEL | Praticamente igual |
| first_person_ratio | **-0.049** | NEGLIGENCIÃVEL | Sem diferenÃ§a prÃ¡tica |

**ConclusÃ£o:** 6 caracterÃ­sticas tÃªm efeito grande, 1 mÃ©dio, 1 pequeno, 2 negligenciÃ¡veis.

---

## 5. Leituras Sugeridas por TÃ³pico

### 5.1 Fundamentos de Estilometria
1. **Mosteller & Wallace (1964)** - "Inference and Disputed Authorship: The Federalist"
   - ğŸ“˜ Livro clÃ¡ssico, primeiro uso computacional para atribuiÃ§Ã£o de autoria
   - **O que vocÃª aprenderÃ¡:** Uso de palavras funcionais, mÃ©todo bayesiano

2. **Burrows (2002)** - "'Delta': A Measure of Stylistic Difference"
   - ğŸ“„ Paper curto (20 pÃ¡ginas), altamente citado
   - **O que vocÃª aprenderÃ¡:** Medida Delta para comparar estilos, base da estilometria moderna

3. **Stamatatos (2009)** - "A survey of modern authorship attribution methods"
   - ğŸ“„ Survey completo (20 pÃ¡ginas), excelente panorama
   - **O que vocÃª aprenderÃ¡:** Todos os mÃ©todos de atribuiÃ§Ã£o de autoria atÃ© 2009

### 5.2 DetecÃ§Ã£o de LLMs (Recente, 2023-2025)
4. **Herbold et al. (2023)** - "A Large-Scale Comparison of Human-Written Versus ChatGPT-Generated Essays"
   - ğŸ“„ Scientific Data (Nature), peer-reviewed
   - **O que vocÃª aprenderÃ¡:** 31 caracterÃ­sticas, Random Forest, 81-98% acurÃ¡cia

5. **Zaitsu & Jin (2023)** - "Distinguishing ChatGPT-generated and human-written papers through Japanese stylometric analysis"
   - ğŸ“„ PLOS One, aplicaÃ§Ã£o em japonÃªs
   - **O que vocÃª aprenderÃ¡:** ValidaÃ§Ã£o cross-linguistic, 100% precisÃ£o

6. **Przystalski et al. (2025)** - "Stylometry recognizes human and LLM-generated texts in short samples"
   - ğŸ“„ Expert Systems with Applications, mais recente
   - **O que vocÃª aprenderÃ¡:** StyloMetrix, centenas de features, performance em textos curtos

### 5.3 EstatÃ­stica NÃ£o-ParamÃ©trica
7. **Mann & Whitney (1947)** - "On a test of whether one of two random variables is stochastically larger"
   - ğŸ“„ Paper original do teste U, matemÃ¡tica pesada
   - **Alternativa mais acessÃ­vel:** Siegel & Castellan (1988) - Livro didÃ¡tico

8. **Cliff (1993)** - "Dominance statistics: Ordinal analyses to answer ordinal questions"
   - ğŸ“„ Psychological Bulletin, introduz Cliff's delta
   - **O que vocÃª aprenderÃ¡:** Por que delta Ã© melhor que Cohen's d para dados ordinais

9. **Benjamini & Hochberg (1995)** - "Controlling the false discovery rate"
   - ğŸ“„ JRSS, correÃ§Ã£o FDR (mais liberal que Bonferroni)
   - **O que vocÃª aprenderÃ¡:** CorreÃ§Ã£o para mÃºltiplos testes, controle de FDR

### 5.4 LÃ³gica Fuzzy
10. **Zadeh (1965)** - "Fuzzy sets"
    - ğŸ“„ Information and Control, paper fundacional
    - **O que vocÃª aprenderÃ¡:** Conceito de pertinÃªncia gradual, funÃ§Ãµes de pertinÃªncia

11. **Klir & Yuan (1995)** - "Fuzzy Sets and Fuzzy Logic: Theory and Applications"
    - ğŸ“˜ Livro-texto completo (574 pÃ¡ginas), referÃªncia definitiva
    - **O que vocÃª aprenderÃ¡:** Tudo sobre fuzzy - teoria, aplicaÃ§Ãµes, exemplos

12. **Pedrycz (1994)** - "Why triangular membership functions?"
    - ğŸ“„ Fuzzy Sets and Systems, justificativa teÃ³rica
    - **O que vocÃª aprenderÃ¡:** Por que funÃ§Ãµes triangulares sÃ£o boas o suficiente

13. **Ross (2010)** - "Fuzzy Logic with Engineering Applications" (3rd ed.)
    - ğŸ“˜ Livro didÃ¡tico (600 pÃ¡ginas), foco prÃ¡tico
    - **O que vocÃª aprenderÃ¡:** ImplementaÃ§Ã£o de sistemas fuzzy, exemplos de cÃ³digo

14. **Takagi & Sugeno (1985)** - "Fuzzy identification of systems"
    - ğŸ“„ IEEE Trans. SMC, sistemas Takagi-Sugeno
    - **O que vocÃª aprenderÃ¡:** Sistemas fuzzy com consequentes lineares (ou constantes)

### 5.5 AnÃ¡lise Multivariada
15. **Jolliffe (2002)** - "Principal Component Analysis"
    - ğŸ“˜ Livro completo sobre PCA (488 pÃ¡ginas)
    - **O que vocÃª aprenderÃ¡:** MatemÃ¡tica do PCA, interpretaÃ§Ã£o de loadings

16. **Fisher (1936)** - "The use of multiple measurements in taxonomic problems"
    - ğŸ“„ Paper original da LDA, exemplo da Iris
    - **O que vocÃª aprenderÃ¡:** SeparaÃ§Ã£o de classes usando projeÃ§Ã£o linear

17. **Hosmer & Lemeshow (2013)** - "Applied Logistic Regression" (3rd ed.)
    - ğŸ“˜ Livro didÃ¡tico (500 pÃ¡ginas), padrÃ£o-ouro
    - **O que vocÃª aprenderÃ¡:** RegressÃ£o logÃ­stica detalhada, interpretaÃ§Ã£o de odds ratios

---

## 6. Conceitos a Revisar Antes de Ler os Papers

### 6.1 EstatÃ­stica BÃ¡sica
- [ ] MÃ©dia, mediana, desvio padrÃ£o
- [ ] DistribuiÃ§Ãµes (normal, nÃ£o-normal)
- [ ] Teste de hipÃ³tese (Hâ‚€, Hâ‚, p-valor)
- [ ] SignificÃ¢ncia estatÃ­stica vs significÃ¢ncia prÃ¡tica
- [ ] CorrelaÃ§Ã£o de Pearson

### 6.2 EstatÃ­stica NÃ£o-ParamÃ©trica
- [ ] DiferenÃ§a entre testes paramÃ©tricos e nÃ£o-paramÃ©tricos
- [ ] Mann-Whitney U test (Wilcoxon rank-sum)
- [ ] Quando usar nÃ£o-paramÃ©trico (violaÃ§Ã£o de normalidade, outliers)

### 6.3 Machine Learning BÃ¡sico
- [ ] ClassificaÃ§Ã£o binÃ¡ria (duas classes)
- [ ] Treino/teste/validaÃ§Ã£o
- [ ] Overfitting
- [ ] Cross-validation (k-fold, stratified)
- [ ] MÃ©tricas: acurÃ¡cia, precisÃ£o, recall, F1, AUC-ROC

### 6.4 Ãlgebra Linear
- [ ] Vetores e matrizes
- [ ] Produto escalar
- [ ] ProjeÃ§Ã£o
- [ ] Autovalores e autovetores (para PCA)

### 6.5 LÃ³gica Fuzzy
- [ ] DiferenÃ§a entre lÃ³gica crisp (0/1) e fuzzy ([0,1])
- [ ] FunÃ§Ã£o de pertinÃªncia
- [ ] Operadores fuzzy (AND, OR, NOT)
- [ ] Sistemas de inferÃªncia fuzzy

---

## 7. PrÃ³ximos Passos Neste Guia

Esta Ã© a **Parte 1: VisÃ£o Geral**. Os prÃ³ximos documentos detalharÃ£o:

- **GUIA_02_CARACTERISTICAS.md** - ImplementaÃ§Ã£o detalhada das 10 caracterÃ­sticas
- **GUIA_03_ESTATISTICA.md** - Testes estatÃ­sticos passo a passo
- **GUIA_04_CLASSIFICADORES.md** - PCA, LDA, RegressÃ£o LogÃ­stica explicados
- **GUIA_05_FUZZY.md** - LÃ³gica fuzzy e funÃ§Ãµes de pertinÃªncia
- **GUIA_06_VALIDACAO.md** - Cross-validation e mÃ©tricas de avaliaÃ§Ã£o
- **GUIA_07_RESULTADOS.md** - InterpretaÃ§Ã£o dos resultados
- **GUIA_08_PERGUNTAS_DEFESA.md** - Perguntas esperadas na defesa com respostas

---

**PrÃ³ximo:** [GUIA_02_CARACTERISTICAS.md](GUIA_02_CARACTERISTICAS.md) - ImplementaÃ§Ã£o Detalhada das CaracterÃ­sticas EstilomÃ©tricas
